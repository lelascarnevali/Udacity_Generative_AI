{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42707c34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets==\"3.2.0\"\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24e80bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f182d899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('gpt2',\n",
    "                                                      num_labels=2,\n",
    "                                                      id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "                                                      label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed83d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    inputs[\"labels\"] = examples[\"label\"]  # Usar labels originais\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1fecc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "\n",
    "\n",
    "eval_dataset = concatenate_datasets([\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    tokenized_datasets[\"test\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a361f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d1267a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_639/3060970571.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 04:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>0.397220</td>\n",
       "      <td>0.819418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.503573418389099, metrics={'train_runtime': 272.7957, 'train_samples_per_second': 31.269, 'train_steps_per_second': 1.958, 'total_flos': 557215320637440.0, 'train_loss': 0.503573418389099, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=3e-4,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=1,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "416f7e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Resultados da Avalia√ß√£o Antes do Fine-Tuning ===\n",
      "EVAL_LOSS                : 0.3972\n",
      "EVAL_ACCURACY            : 0.8194\n",
      "EVAL_RUNTIME             : 18.6913\n",
      "EVAL_SAMPLES_PER_SECOND  : 114.0640\n",
      "EVAL_STEPS_PER_SECOND    : 7.1690\n",
      "EPOCH                    : 1\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "prior_evaluate = trainer.evaluate()\n",
    "\n",
    "print(\"\\n=== Resultados da Avalia√ß√£o Antes do Fine-Tuning ===\")\n",
    "for key, value in prior_evaluate.items():\n",
    "    # Formata√ß√£o especial para valores num√©ricos\n",
    "    if isinstance(value, float):\n",
    "        if key == \"epoch\":\n",
    "            print(f\"{key.upper():<25}: {int(value)}\")\n",
    "        else:\n",
    "            print(f\"{key.upper():<25}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key.upper():<25}: {value}\")\n",
    "\n",
    "print(\"===================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bab0db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predictions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cool gadgets and creatures keep this fresh . not as good as the original , but what is . . .</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>an awful movie that will only satisfy the most emotionally malleable of filmgoers .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>. . . you can be forgiven for realizing that you've spent the past 20 minutes looking at your watch and waiting for frida to just die already .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>though uniformly well acted , especially by young ballesta and galan ( a first-time actor ) , writer/director achero manas's film is schematic and obvious .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>absolutely ( and unintentionally ) terrifying .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shanghai ghetto , much stranger than any fiction , brings this unknown slice of history affectingly to life .</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>while hoffman's performance is great , the subject matter goes nowhere .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>works because we're never sure if ohlinger's on the level or merely a dying , delusional man trying to get into the history books before he croaks .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>as a science fiction movie , \" minority report \" astounds .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what one is left with , even after the most awful acts are committed , is an overwhelming sadness that feels as if it has made its way into your very bloodstream .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                  text  \\\n",
       "0                                                                         cool gadgets and creatures keep this fresh . not as good as the original , but what is . . .   \n",
       "1                                                                                  an awful movie that will only satisfy the most emotionally malleable of filmgoers .   \n",
       "2                      . . . you can be forgiven for realizing that you've spent the past 20 minutes looking at your watch and waiting for frida to just die already .   \n",
       "3         though uniformly well acted , especially by young ballesta and galan ( a first-time actor ) , writer/director achero manas's film is schematic and obvious .   \n",
       "4                                                                                                                      absolutely ( and unintentionally ) terrifying .   \n",
       "5                                                        shanghai ghetto , much stranger than any fiction , brings this unknown slice of history affectingly to life .   \n",
       "6                                                                                             while hoffman's performance is great , the subject matter goes nowhere .   \n",
       "7                 works because we're never sure if ohlinger's on the level or merely a dying , delusional man trying to get into the history books before he croaks .   \n",
       "8                                                                                                          as a science fiction movie , \" minority report \" astounds .   \n",
       "9  what one is left with , even after the most awful acts are committed , is an overwhelming sadness that feels as if it has made its way into your very bloodstream .   \n",
       "\n",
       "   predictions  labels  \n",
       "0            1       1  \n",
       "1            0       0  \n",
       "2            0       0  \n",
       "3            0       0  \n",
       "4            0       0  \n",
       "5            1       1  \n",
       "6            0       0  \n",
       "7            0       1  \n",
       "8            0       1  \n",
       "9            0       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "results = trainer.predict(eval_dataset)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"text\": [item[\"text\"] for item in eval_dataset],\n",
    "    \"predictions\": results.predictions.argmax(axis=1),\n",
    "    \"labels\": results.label_ids,\n",
    "})\n",
    "\n",
    "# Fun√ß√£o para selecionar amostra aleat√≥ria\n",
    "def show_random_samples(df, quantity: int, random_seed: int = 42):\n",
    "    return df.sample(n=quantity, random_state=random_seed).reset_index(drop=True)\n",
    "\n",
    "# Quantidade de itens para exibir\n",
    "qtd_item = 10\n",
    "\n",
    "# Exibir amostra aleat√≥ria\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "show_random_samples(df, qtd_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9199a68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predictions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a mischievous visual style and oodles of charm make 'cherish' a very good ( but not great ) movie .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the importance of being earnest , so thick with wit it plays like a reading from bartlett's familiar quotations</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>made for teens and reviewed as such , this is recommended only for those under 20 years of age . . . and then only as a very mild rental .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imagine o . henry's &lt;b&gt;the gift of the magi&lt;/b&gt; relocated to the scuzzy underbelly of nyc's drug scene . merry friggin' christmas !</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nothing short of wonderful with its ten-year-old female protagonist and its steadfast refusal to set up a dualistic battle between good and evil .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>like the tuck family themselves , this movie just goes on and on and on and on</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a film that plays things so nice 'n safe as to often play like a milquetoast movie of the week blown up for the big screen .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>more intellectually scary than dramatically involving .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>narc is all menace and atmosphere .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>there are many definitions of 'time waster' but this movie must surely be one of them .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                 text  \\\n",
       "0                                                 a mischievous visual style and oodles of charm make 'cherish' a very good ( but not great ) movie .   \n",
       "1                                     the importance of being earnest , so thick with wit it plays like a reading from bartlett's familiar quotations   \n",
       "2          made for teens and reviewed as such , this is recommended only for those under 20 years of age . . . and then only as a very mild rental .   \n",
       "3                 imagine o . henry's <b>the gift of the magi</b> relocated to the scuzzy underbelly of nyc's drug scene . merry friggin' christmas !   \n",
       "4  nothing short of wonderful with its ten-year-old female protagonist and its steadfast refusal to set up a dualistic battle between good and evil .   \n",
       "5                                                                      like the tuck family themselves , this movie just goes on and on and on and on   \n",
       "6                        a film that plays things so nice 'n safe as to often play like a milquetoast movie of the week blown up for the big screen .   \n",
       "7                                                                                             more intellectually scary than dramatically involving .   \n",
       "8                                                                                                                 narc is all menace and atmosphere .   \n",
       "9                                                             there are many definitions of 'time waster' but this movie must surely be one of them .   \n",
       "\n",
       "   predictions  labels  \n",
       "0            0       1  \n",
       "1            0       1  \n",
       "2            0       1  \n",
       "3            0       1  \n",
       "4            0       1  \n",
       "5            1       0  \n",
       "6            1       0  \n",
       "7            1       0  \n",
       "8            1       0  \n",
       "9            1       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_discrepancies(results_df, num_samples=5):\n",
    "    discrepancies = results_df[results_df['labels'] != results_df['predictions']]\n",
    "    \n",
    "    if discrepancies.empty:\n",
    "        return pd.DataFrame({'Message': ['‚úÖ No discrepancies found']})\n",
    "    \n",
    "    total = len(discrepancies)\n",
    "    samples = min(num_samples, total)\n",
    "    \n",
    "    if total <= samples * 2:\n",
    "        return discrepancies.reset_index(drop=True)\n",
    "    \n",
    "    head_df = discrepancies.head(samples).copy()\n",
    "    tail_df = discrepancies.tail(samples).copy()\n",
    "    \n",
    "    return pd.concat([head_df, tail_df]).reset_index(drop=True)\n",
    "\n",
    "get_discrepancies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc373f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",                  # Task espec√≠fica para classifica√ß√£o\n",
    "    r=8,                                  # Rank da decomposi√ß√£o LoRA\n",
    "    lora_alpha=64,                        # Fator de escala\n",
    "    lora_dropout=0.2,                     # Dropout para regulariza√ß√£o\n",
    "    target_modules=['c_attn', 'c_proj'],  # M√≥dulos do GPT-2 para aplicar LoRA\n",
    "    bias=\"none\"                           # Estrat√©gia para bias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 812,544 || all params: 125,253,888 || trainable%: 0.6487\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./peft_results\",\n",
    "    learning_rate=3e-4,    \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=1,    \n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c95b89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 03:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.434200</td>\n",
       "      <td>0.453732</td>\n",
       "      <td>0.829737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.2785607654503669, metrics={'train_runtime': 188.6974, 'train_samples_per_second': 45.205, 'train_steps_per_second': 2.83, 'total_flos': 562538328883200.0, 'train_loss': 0.2785607654503669, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77b60d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/gpt2-rotten-tomatoes-lora/tokenizer_config.json',\n",
       " './model/gpt2-rotten-tomatoes-lora/special_tokens_map.json',\n",
       " './model/gpt2-rotten-tomatoes-lora/vocab.json',\n",
       " './model/gpt2-rotten-tomatoes-lora/merges.txt',\n",
       " './model/gpt2-rotten-tomatoes-lora/added_tokens.json',\n",
       " './model/gpt2-rotten-tomatoes-lora/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.save_pretrained(\"./model/gpt2-rotten-tomatoes-lora\")\n",
    "tokenizer.save_pretrained(\"./model/gpt2-rotten-tomatoes-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained('gpt2').to(device)\n",
    "peft_loaded = PeftModel.from_pretrained(loaded_model, \"./model/gpt2-rotten-tomatoes-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Performance Comparison ===\n",
      "Metric                    | Original   | PEFT      \n",
      "--------------------------------------------------\n",
      "EVAL_LOSS                 | 0.3972     | 0.4537    \n",
      "EVAL_ACCURACY             | 0.8194     | 0.8297    \n",
      "EVAL_RUNTIME              | 18.6913    | 20.9032   \n",
      "EVAL_SAMPLES_PER_SECOND   | 114.0640   | 101.9940  \n",
      "EVAL_STEPS_PER_SECOND     | 7.1690     | 6.4100    \n",
      "EPOCH                     | 1.0000     | 1.0000    \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Performance Comparison ===\")\n",
    "print(f\"{'Metric':<25} | {'Original':<10} | {'PEFT':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Ensure metrics exist in both results\n",
    "peft_evaluate = peft_trainer.evaluate()\n",
    "\n",
    "for key in prior_evaluate:\n",
    "    if key not in peft_evaluate:\n",
    "        continue\n",
    "    \n",
    "    # Format original values\n",
    "    original_val = prior_evaluate[key]\n",
    "    formatted_original = f\"{original_val:.4f}\" if isinstance(original_val, float) else str(original_val)\n",
    "    \n",
    "    # Format PEFT values\n",
    "    peft_val = peft_evaluate[key]\n",
    "    formatted_peft = f\"{peft_val:.4f}\" if isinstance(peft_val, float) else str(peft_val)\n",
    "    \n",
    "    print(f\"{key.upper():<25} | {formatted_original:<10} | {formatted_peft:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe9d85",
   "metadata": {},
   "source": [
    "## Key Insights:\n",
    "\n",
    "### Accuracy-Loss Paradox:\n",
    "\n",
    "PEFT shows **+1.03% better accuracy** (82.97% vs 81.94%) despite +14.22% higher loss (0.4537 vs 0.3972)\n",
    "\n",
    "Common in scenarios where:\n",
    "\n",
    "    * Model makes more confident wrong predictions (increases loss)\n",
    "    * Correct predictions have lower confidence margins\n",
    "\n",
    "### Computational Efficiency:\n",
    "\n",
    "Throughput **decreased by -8.73%** (104.1 samples/sec vs 114.06)\n",
    "Due to adapter operations in PEFT adding computational overhead\n",
    "\n",
    "### Training Dynamics:\n",
    "\n",
    "Both models trained for 1 epoch\n",
    "\n",
    "PEFT achieves better accuracy with only ~0.2% of parameters updated\n",
    "Traditional method shows better loss but lower accuracy (potential overfitting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
