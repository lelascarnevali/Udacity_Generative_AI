{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from helper import (\n",
    "    start_time,\n",
    "    time_since,\n",
    "    ShakespeareDataset,\n",
    "    TokenMapping,\n",
    "    build_model,\n",
    "    next_token,\n",
    "    # Character-based helpers\n",
    "    encode_text,\n",
    "    # Subword-based helpers\n",
    "    encode_text_from_tokenizer,\n",
    "    tokenize_text_from_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Deterministic training\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Attempt GPU; if not, stay on CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text file: 50,085\n"
     ]
    }
   ],
   "source": [
    "# Reduced data to make it manageable for smaller systems\n",
    "DATA_FILE: str = '../data/shakespeare_small.txt'\n",
    "\n",
    "with open(DATA_FILE, 'r') as data_file:\n",
    "    raw_text = data_file.read()\n",
    "\n",
    "print(f'Number of characters in text file: {len(raw_text):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Based Text Generation\n",
    "\n",
    "The first model you'll build for text generation will use character-based\n",
    "tokens.\n",
    "\n",
    "Each token will be a single character from the text and the model will learn\n",
    "to predict the next character (a token).\n",
    "\n",
    "To generate text, the model will take in a new string,\n",
    "character-by-character, and then generate a new likely character based on the\n",
    "past input. Then the model will take into account that new character and\n",
    "generate the following character and so on and so on until the model has\n",
    "produced a set number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # TODO: Normalize incoming text; can be multiple actions\n",
    "    # Remover acentos\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    # Converter para minúsculas\n",
    "    text = text.lower()\n",
    "    # Remover pontuação\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Substituir múltiplos espaços por um único espaço\n",
    "    normalized_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen before we proceed any further hear me speak all speak speak first citizen you are all resolved rather to die than to famish all resolved resolved first citizen first you know caius marcius is chief enemy to the people all we knowt we knowt first citizen let us kill him and well have corn at our own price ist a verdict all no more talking ont let it be done away away second citizen one word good citizens first citizen we are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your text normalized the way you expected?\n",
    "# Only the first 500 characters of the original text\n",
    "normalized_text = normalize_text(raw_text[:500])\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> str | list[str]:\n",
    "    # TODO: Pretokenize normalized text into character strings\n",
    "    smaller_pieces = list(text)\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ' ', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', ' ', 'a', 'l', 'l', ' ', 's', 'p', 'e', 'a', 'k', ' ', 's', 'p', 'e', 'a', 'k', ' ', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ' ', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ' ', 'f', 'i', 'r', 's', 't', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 'a', 'l', 'l', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', 't', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', 't', ' ', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ' ', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ' ', 'a', 'n', 'd', ' ', 'w', 'e', 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', ' ', 'i', 's', 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', ' ', 'a', 'l', 'l', ' ', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', 't', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ' ', 'a', 'w', 'a', 'y', ' ', 'a', 'w', 'a', 'y', ' ', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ' ', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', ' ', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ' ', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r']\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your (normalized) text pretokenized the way you expected?\n",
    "# Only the first 500 characters of the original text\n",
    "pretokenized_text = pretokenize_text(normalized_text)\n",
    "print(pretokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normalization and pretokenization steps\n",
    "def tokenize_text(text: str) -> str | list[str]:\n",
    "    normalized_text: str = normalize_text(text)\n",
    "    pretokenized_text: str | list[str] = pretokenize_text(normalized_text)\n",
    "    # Characters are already tokens so pretokenized text is already tokenized\n",
    "    tokenized_text = pretokenized_text\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ' ', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', ' ', 'a', 'l', 'l', ' ', 's', 'p', 'e', 'a', 'k', ' ', 's', 'p', 'e', 'a', 'k', ' ', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ' ', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ' ', 'f', 'i', 'r', 's', 't', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 'a', 'l', 'l', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', 't', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', 't', ' ', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ' ', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ' ', 'a', 'n', 'd', ' ', 'w', 'e', 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', ' ', 'i', 's', 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', ' ', 'a', 'l', 'l', ' ', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', 't', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ' ', 'a', 'w', 'a', 'y', ' ', 'a', 'w', 'a', 'y', ' ', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ' ', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', ' ', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ' ', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r']\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your tokenized text the way you expected?\n",
    "tokenized_text = tokenize_text(raw_text[:500])\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "We'll skip postprocessing since we don't have any special tokens we want to\n",
    "consider for our task here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)\n",
    "\n",
    "We have `encode_text()` from our helper module that can encode our text based on\n",
    "our tokenization process from our created `tokenize_text()` function.\n",
    "\n",
    "This will also provide us with `character_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, character_mapping = encode_text(raw_text, tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 47,306 characters\n"
     ]
    }
   ],
   "source": [
    "n_tokens = character_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of characters\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_char()` function will use your tokenizer and NLP model to\n",
    "generate new text token-by-token (character-by-character in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_chars` parameter to tell the function how many tokens\n",
    "(characters in this case) to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_char(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = character_mapping,\n",
    "    num_chars: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Uses your character-based tokenizer\n",
    "    tokenized_text: list[str] = tokenize_text(input_str)\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_chars):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_char = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_char)\n",
    "    # Returns input string plus the full generated string (of generated tokens)\n",
    "    full_text = ''.join(tokenized_text + generated_tokens)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our character-based\n",
    "tokenizer) and attempt to predict the next token. Over time, the model should\n",
    "hopefully get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 2.326095863753307\n",
      "[00m 4.3s (0 0.0) 1.9516]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bes and kerrips agest a rie thenrtinh bm onld bugt frakeerdeprt other doo whthut necanges seed dopburs\n",
      "Epoch 2/25, Loss: 2.03247764815497\n",
      "[00m 8.4s (1 4.0) 1.6832]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bey caavat it thecinin shome thake ancins bu sere the tiin enart to projo of yo the plecence wod the b\n",
      "Epoch 3/25, Loss: 1.9417770695299192\n",
      "[00m 12.5s (2 8.0) 1.5783]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to betharto literen the nous recoos and toolr mave wlemuldse the prent sowy haring en we hes coo for and \n",
      "Epoch 4/25, Loss: 1.8860080784970594\n",
      "[00m 16.7s (3 12.0) 1.5097]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be thive heat thesery sooff the susus be to for the pwor rese dowc your soos knomt on thoo cond and so\n",
      "Epoch 5/25, Loss: 1.8461697870242901\n",
      "[00m 20.8s (4 16.0) 1.4613]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to benders so kedy at fir latch i fwatrew his nelvatiors teake got ender to have hurmest be to uncound ea\n",
      "Epoch 6/25, Loss: 1.8156488386962024\n",
      "[00m 24.8s (5 20.0) 1.4323]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bear ourped volopl to asprumons mereion setye hest vell he fren larothitler a chaing the apktorour pro\n",
      "Epoch 7/25, Loss: 1.7916467297867607\n",
      "[00m 28.9s (6 24.0) 1.4124]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bent the their be it is indangusy a st cangreire of let shall the rarciust mids be anst dos him good o\n",
      "Epoch 8/25, Loss: 1.7720040003888824\n",
      "[00m 32.9s (7 28.000000000000004) 1.3979]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be to sose wopt out off but and your sis love niso live on uplle eveabend do the fraywuengs teresore o\n",
      "Epoch 9/25, Loss: 1.7552380852996092\n",
      "[00m 36.7s (8 32.0) 1.3895]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be txeike thinday  ase nobsnedagr a wowar on thee stose afgither hure slalloks seplent if that rome re\n",
      "Epoch 10/25, Loss: 1.7406965987769452\n",
      "[00m 40.5s (9 36.0) 1.3830]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be prow not titroble first nere esosing thas wer ingess he ese tho i will mjonger tere comi shong an a\n",
      "Epoch 11/25, Loss: 1.7280505745594814\n",
      "[00m 44.2s (10 40.0) 1.3768]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bent ank itsure you bun dave and to vive ats and titow and tled do out whels neid trut it of mess he d\n",
      "Epoch 12/25, Loss: 1.7169316087588566\n",
      "[00m 48.0s (11 44.0) 1.3716]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bert my fot crattrer knour berely thatle flvestain at and well not grance fay may to henefly hinher up\n",
      "Epoch 13/25, Loss: 1.7070101419708241\n",
      "[00m 51.7s (12 48.0) 1.3678]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to belsly eed ow in the hard they boding we hing you day gentiter their pray theikes for of eseneport not\n",
      "Epoch 14/25, Loss: 1.6981156320791284\n",
      "[00m 55.6s (13 52.0) 1.3645]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beatoul bar a haved blezes at that vales of it benour you to or phore him hin our he that wabthe hore \n",
      "Epoch 15/25, Loss: 1.6899956588170848\n",
      "[00m 59.4s (14 56.00000000000001) 1.3601]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be is wolanus to ill camther livegat he wle let dus and fadunter cefgrinest dered tith untingitelly re\n",
      "Epoch 16/25, Loss: 1.682673621242198\n",
      "[01m 3.2s (15 60.0) 1.3555]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bettery he as our as ben for valeth your thougd to gorice as has and sing fices fearth in hate the vop\n",
      "Epoch 17/25, Loss: 1.676088089468676\n",
      "[01m 7.1s (16 64.0) 1.3512]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be for exenter put to hath thuers that thered actose and and thouff intle his the peramoured in and to\n",
      "Epoch 18/25, Loss: 1.670129644854627\n",
      "[01m 11.1s (17 68.0) 1.3466]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be to raseqyather brest your hed in were the all seet makg i daik and the rime antold to as woundeds l\n",
      "Epoch 19/25, Loss: 1.6646797990121442\n",
      "[01m 14.9s (18 72.0) 1.3407]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to belont whose waged gese themee and tham sherging and coufnd evun cane nows efretion were acandash ingl\n",
      "Epoch 20/25, Loss: 1.6597056506612466\n",
      "[01m 18.8s (19 76.0) 1.3357]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to better youlf inswerels ppedes constrybertus affixius for becoting not i somint hors what and to hasbon\n",
      "Epoch 21/25, Loss: 1.6551476526647524\n",
      "[01m 22.6s (20 80.0) 1.3322]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bellne be is placionllow thankour he plood eved fart teed and trey knther he corsionlich heth to garat\n",
      "Epoch 22/25, Loss: 1.6509236077171863\n",
      "[01m 26.4s (21 84.0) 1.3298]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to best a just for and char elvery bedert devedice and to no the sicinius malink to emve retaids thib he \n",
      "Epoch 23/25, Loss: 1.6469580927140335\n",
      "[01m 30.2s (22 88.0) 1.3284]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be aufice paly i deved me wars plutuy not their henerioli seeath thou tis buhition indeed to tosest th\n",
      "Epoch 24/25, Loss: 1.6432151220164215\n",
      "[01m 34.0s (23 92.0) 1.3277]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bemnil nuptiors if all have hall offids i sicines as a the forrien to endencle for that of firfie trav\n",
      "Epoch 25/25, Loss: 1.6397379020554126\n",
      "[01m 37.8s (24 96.0) 1.3273]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bear thoughters spong time to withat will as marcius wer we to be eseecitgary lave waspobe a debesenti\n"
     ]
    }
   ],
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    gen_output = generate_text_by_char(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        num_chars=100,\n",
    "    )\n",
    "    print(gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_chars`) to\n",
    "generate and observe how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to becond menenius honkning not fot or rith hove whasved word let where a bround your a most hothertus wa\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_char(\n",
    "    input_str='To be or not to be',\n",
    "    model=model,\n",
    "    num_chars=100,\n",
    "    temperature=1.0,\n",
    "    topk=None,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Text Generation\n",
    "\n",
    "The next model you'll build will use subword-tokenization instead of \n",
    "characters-based token to train a model and ultimately generate new text\n",
    "token-by-token.\n",
    "\n",
    "Although this could be done by creating your own tokenizer, you'll use\n",
    "Hugging Face to use a pretrained tokenizer to tokenize the data.\n",
    "\n",
    "After training the model with subword tokens, \n",
    "the model will take in a new string, token-by-token, and then generate a new\n",
    "token (subword).\n",
    "The model will continue producing new subword tokens based on the input text\n",
    "and already produced tokens until a set number of tokens have been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Tokenizer\n",
    "\n",
    "> NOTE:\n",
    "> \n",
    "> You can load another model outside of these choices but the model\n",
    "> will have to be downloaded and may or may not be effective.\n",
    ">\n",
    "> If you'd like to explore more, here's a link to you might want to start with\n",
    "> of different available pretrained models on Hugging Face:\n",
    "> https://huggingface.co/models?pipeline_tag=text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose a pretrained tokenizer to use:\n",
    "\n",
    "# Docs: https://huggingface.co/xlm-roberta-base\n",
    "model_name = 'xlm-roberta-base'\n",
    "# DOCS: https://huggingface.co/bert-base-cased\n",
    "# model_name = 'bert-base-cased'\n",
    "# DOCS: https://huggingface.co/bert-base-uncased \n",
    "# model_name = 'bert-base-uncased'\n",
    "\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)\n",
    "\n",
    "We have `encode_text_from_tokenizer()` from our helper module that can encode\n",
    "our text based on our tokenization process from our tokenizer `my_tokenizer`.\n",
    "\n",
    "This will also provide us with `token_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, token_mapping = encode_text_from_tokenizer(\n",
    "    text=raw_text,\n",
    "    tokenizer=my_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 14,374 tokens\n"
     ]
    }
   ],
   "source": [
    "n_tokens = token_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of tokens\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_subword()` function will use your chosen tokenizer and the\n",
    "NLP model to generate new text token-by-token (subwords in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_tokens` parameter to tell the function how many\n",
    "(subword)tokens to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_subword(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = token_mapping,\n",
    "    tokenizer = my_tokenizer,\n",
    "    num_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Use your chosen subword-tokenizer\n",
    "    tokenized_text = tokenize_text_from_tokenizer(\n",
    "        tokenizer=tokenizer,\n",
    "        text=input_str,\n",
    "    )\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_tokens):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_token = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_token)\n",
    "    # List of all token IDs (input text and generated text)\n",
    "    output_ids = tokenizer.convert_tokens_to_ids(\n",
    "        tokenized_text + generated_tokens\n",
    "    )\n",
    "    # Returns input string plus the full generated string from list of token IDs\n",
    "    full_text = tokenizer.decode(output_ids)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our subword tokenizer)\n",
    "and attempt to predict the next token. Over time, the model should hopefully\n",
    "get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 6.724869424356915\n",
      "[00m 1.4s (0 0.0) 5.7264]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to beians alone. his, w best' true, party have As patiencebble endur his  bear.,, Senator muni proud;.t are'\n",
      "Epoch 2/25, Loss: 6.079475733113449\n",
      "[00m 2.7s (1 4.0) 5.2852]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be he Which his people,.GI, this Ti non behind I Bearss fac:''siss you hims hist no the\n",
      "Epoch 3/25, Loss: 5.7861398588575605\n",
      "[00m 4.0s (2 8.0) 4.8973]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to bes his and distribution lie was thatamour thenNUSy?; blood Withstin to heaven. noon, audit naft will Should of common he Capitol\n",
      "Epoch 4/25, Loss: 5.513100803562156\n",
      "[00m 5.3s (3 12.0) 4.6461]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be burde their their nor talk came good ta side your too Than Fare un in his fai worth came bear isto yet face, greater mill men, I\n",
      "Epoch 5/25, Loss: 5.282514653386412\n",
      "[00m 6.5s (4 16.0) 4.4637]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be refuse- Capitol livessour themst du, they proved heavy tireboard welcomein they entrece, I theck, decline. BRUUMUS\n",
      "Epoch 6/25, Loss: 5.089779053605214\n",
      "[00m 7.8s (5 20.0) 4.3172]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be hon a are Thirdd inword as please! atgus is the Anti agentsIUS: O, mada themsur for their heardheadourif And\n",
      "Epoch 7/25, Loss: 4.924483072518771\n",
      "[00m 9.1s (6 24.0) 4.1932]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to beshow base prager Coriola and in a dispositionmy, I shall consuls, and deingberdar the en examine rbeys aNUS\n",
      "Epoch 8/25, Loss: 4.778197146205435\n",
      "[00m 10.4s (7 28.000000000000004) 4.0860]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be mat Senator: Your will arm. SICINIUS:u la vente with fi those to yourwordri carb ale: 'head only choice, To\n",
      "Epoch 9/25, Loss: 4.646272064051809\n",
      "[00m 11.7s (8 32.0) 3.9920]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be counsel objects in coming re strike than. AUFIDIUS: Be at wiLIA:bles. MENENIUS:lander has these enough:\n",
      "Epoch 10/25, Loss: 4.52492436746712\n",
      "[00m 13.0s (9 36.0) 3.9076]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be Bruted the see than an Tarmb with them. VIRGI have'd to good the windives to willinius. AUFIDIUS:\n",
      "Epoch 11/25, Loss: 4.411936203461713\n",
      "[00m 14.3s (10 40.0) 3.8284]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be myest, how here they take carry love the kind is tribun commandIDI:d to theks!. Secondoli'd to show Senator:\n",
      "Epoch 12/25, Loss: 4.306242009844706\n",
      "[00m 15.5s (11 44.0) 3.7554]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be one lo but me Welcome should to take the flo?, art, dis ste: SI withinar he for my times, rusdle: what hat\n",
      "Epoch 13/25, Loss: 4.206992535920345\n",
      "[00m 16.8s (12 48.0) 3.6890]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be  him, and your worship or Himsi bear remember; And that hers, when sooctor?, the matter war' theide not\n",
      "Epoch 14/25, Loss: 4.11343510910238\n",
      "[00m 18.1s (13 52.0) 3.6276]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be tren, the bear, giving prettymos the leg, he did wheels, that I haveken Rome ex but per e grati that anystin k\n",
      "Epoch 15/25, Loss: 4.024925333354415\n",
      "[00m 19.4s (14 56.00000000000001) 3.5706]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be moc of youbrow, holding that for Cor entertars clubs I am hand they orcontented the otherrer a pred s this to him the\n",
      "Epoch 16/25, Loss: 3.9409568485013624\n",
      "[00m 20.7s (15 60.0) 3.5178]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be doubte his name, MENENIUS: I Better he hat Second centuri As That must our nuptictor, render you holding me In? First\n",
      "Epoch 17/25, Loss: 3.86119278289693\n",
      "[00m 22.0s (16 64.0) 3.4684]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to beenge part of hot'dcing else are mothers: but one that she give him his schooldro No for's: guide will deries\n",
      "Epoch 18/25, Loss: 3.7853280186387637\n",
      "[00m 23.3s (17 68.0) 3.4225]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be toe the belly proud, come, and, sir.wixt thating and puff of request find him soft my sanc: how nowjocher\n",
      "Epoch 19/25, Loss: 3.7131043091117673\n",
      "[00m 24.5s (18 72.0) 3.3795]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be else cabol content of this him more is flat been estima. Tudeless Senator: the kitchen hat What friends! am, I you remember do\n",
      "Epoch 20/25, Loss: 3.6443355205595362\n",
      "[00m 25.8s (19 76.0) 3.3387]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to belly have some toe. SICINIUS: The instruments grantthpot: come, he tell good successer, t heves. VALERIA\n",
      "Epoch 21/25, Loss: 3.578663330035645\n",
      "[00m 27.1s (20 80.0) 3.2994]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be him's grain him now be give fill every grain theydles, aside, We seeeve; but, that cannot answer, Where not\n",
      "Epoch 22/25, Loss: 3.515903355549598\n",
      "[00m 28.4s (21 84.0) 3.2614]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be harely: Death, we't To, We physical ity sir. VALERIA: Haverons head. BRUTUS: Ver Thiders;\n",
      "Epoch 23/25, Loss: 3.455902632732434\n",
      "[00m 29.7s (22 88.0) 3.2240]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be entombed mali whose I Shall come softitiesus their seeking men therong, pra so Lady rather te loves, to the brother,\n",
      "Epoch 24/25, Loss: 3.3985237793826846\n",
      "[00m 31.0s (23 92.0) 3.1880]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be light that shut to goo Mess make notgging. VOLUMNIA: AUFIDIUS: tired it grace? highest of man!\n",
      "Epoch 25/25, Loss: 3.343721833685724\n",
      "[00m 32.3s (24 96.0) 3.1507]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be mum hate, pra might I in the people theret homes! CORIOLANUS: Pra, for not been him the Capitol, to pra\n"
     ]
    }
   ],
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    output = generate_text_by_subword(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_tokens`)\n",
    "to generate and observe how it does.\n",
    "\n",
    "------------\n",
    "\n",
    "Consider how this model differs from the results from the text generation using\n",
    "the character-based tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be in this man, there pri you too s thyil the bear at and trumpcurdrous toward a Vols won him on mine. Who sensi\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_subword(\n",
    "        input_str='To be or not to be',\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.5,\n",
    "        topk=100,\n",
    "    )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
